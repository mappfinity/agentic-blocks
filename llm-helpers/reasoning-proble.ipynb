{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23006ae-8d0c-4abb-be95-a18d07ca29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reasoning Capability Detection & Caching\n",
    "Unified probe with memory + persistent disk cache with TTL expiration\n",
    "\"\"\"\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "class ReasoningCapabilityProbe:\n",
    "    \"\"\"\n",
    "    Probes LLM for reasoning mode support with intelligent caching.\n",
    "    Maintains an in-memory cache and optional persistent cache file with TTL.\n",
    "\n",
    "    âœ” UNIFIED: Single source of truth for reasoning capability detection\n",
    "    âœ” SMART CACHE: Memory + disk persistence with 24h TTL expiration\n",
    "    âœ” FAST PATH: Pattern matching for known reasoning models (no probe)\n",
    "    âœ” PRODUCTION: Comprehensive error handling, diagnostics, management\n",
    "    \"\"\"\n",
    "\n",
    "    CACHE_DIR = Path(\".cache\")\n",
    "    CACHE_FILE = CACHE_DIR / \"reasoning_capabilities.json\"\n",
    "    CACHE_TTL_HOURS = 480  # Cache validity period\n",
    "\n",
    "    def __init__(self, show_progress: bool = True, use_persistent_cache: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize probe with caching options.\n",
    "\n",
    "        Args:\n",
    "            show_progress: Log probe activities\n",
    "            use_persistent_cache: Save/load cache to disk\n",
    "        \"\"\"\n",
    "        self.show_progress = show_progress\n",
    "        self.use_persistent_cache = use_persistent_cache\n",
    "        self.memory_cache: Dict[str, Dict] = {}\n",
    "\n",
    "        if self.use_persistent_cache:\n",
    "            self._load_persistent_cache()\n",
    "\n",
    "    def _load_persistent_cache(self) -> None:\n",
    "        \"\"\"Load cached capabilities from disk if available.\"\"\"\n",
    "        if not self.CACHE_FILE.exists():\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(self.CACHE_FILE, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                self.memory_cache = data.get(\"capabilities\", {})\n",
    "\n",
    "            if self.show_progress:\n",
    "                print(f\"âœ“ Loaded capability cache: {len(self.memory_cache)} models\")\n",
    "                print(f\"  Cache file: {self.CACHE_FILE}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load persistent cache: {e}\")\n",
    "            self.memory_cache = {}\n",
    "\n",
    "    def _save_persistent_cache(self) -> None:\n",
    "        \"\"\"Save current cache to disk.\"\"\"\n",
    "        if not self.use_persistent_cache:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "            with open(self.CACHE_FILE, \"w\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"ttl_hours\": self.CACHE_TTL_HOURS,\n",
    "                        \"capabilities\": self.memory_cache,\n",
    "                    },\n",
    "                    f,\n",
    "                    indent=2,\n",
    "                )\n",
    "\n",
    "            if self.show_progress:\n",
    "                print(f\"ðŸ’¾ Saved capability cache: {self.CACHE_FILE}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save persistent cache: {e}\")\n",
    "\n",
    "    def _is_cache_valid(self, cache_entry: Dict) -> bool:\n",
    "        \"\"\"Check if a cache entry is still valid (not expired).\"\"\"\n",
    "        if \"timestamp\" not in cache_entry:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            cached_time = datetime.fromisoformat(cache_entry[\"timestamp\"])\n",
    "            expiry_time = cached_time + timedelta(hours=self.CACHE_TTL_HOURS)\n",
    "            is_valid = datetime.now() < expiry_time\n",
    "\n",
    "            if not is_valid and self.show_progress:\n",
    "                print(f\"  Cache entry expired at {expiry_time.isoformat()}\")\n",
    "\n",
    "            return is_valid\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Invalid cache timestamp: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_reasoning_support(\n",
    "            self,\n",
    "            model: str,\n",
    "            temperature: float = 1.0,\n",
    "            context_window: int = 4096,\n",
    "    ) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Check if model supports reasoning mode with caching.\n",
    "\n",
    "        Args:\n",
    "            model: Model name/identifier\n",
    "            temperature: LLM temperature setting\n",
    "            context_window: LLM context window size\n",
    "\n",
    "        Returns:\n",
    "            (supports_reasoning: bool, reason: str)\n",
    "        \"\"\"\n",
    "        model_key = f\"{model}#{temperature}#{context_window}\"\n",
    "\n",
    "        # âœ” CHECK MEMORY CACHE FIRST\n",
    "        if model_key in self.memory_cache:\n",
    "            entry = self.memory_cache[model_key]\n",
    "            if self._is_cache_valid(entry):\n",
    "                if self.show_progress:\n",
    "                    print(f\"ðŸ“¦ Reasoning capability [CACHED]: {model}\")\n",
    "                    print(f\"  Result: {entry['supports_reasoning']}\")\n",
    "                    print(f\"  Reason: {entry['reason']}\")\n",
    "\n",
    "                return entry[\"supports_reasoning\"], entry[\"reason\"]\n",
    "\n",
    "        # âœ” NOT CACHED OR EXPIRED - PROBE THE MODEL\n",
    "        if self.show_progress:\n",
    "            print(f\"ðŸ”ï¸Ž Probing reasoning capability: {model}\")\n",
    "            print(f\"  Temperature: {temperature}\")\n",
    "            print(f\"  Context window: {context_window}\")\n",
    "\n",
    "        supports, reason = self._probe_model(model, temperature, context_window)\n",
    "\n",
    "        # âœ” CACHE THE RESULT\n",
    "        self.memory_cache[model_key] = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"context_window\": context_window,\n",
    "            \"supports_reasoning\": supports,\n",
    "            \"reason\": reason,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        self._save_persistent_cache()\n",
    "\n",
    "        if self.show_progress:\n",
    "            status = \"âœ“ SUPPORTED\" if supports else \"âœ— NOT SUPPORTED\"\n",
    "            print(f\"  {status}: {reason}\")\n",
    "\n",
    "        return supports, reason\n",
    "\n",
    "    def _probe_model(\n",
    "            self, model: str, temperature: float, context_window: int\n",
    "    ) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Attempt to use reasoning mode and detect support.\n",
    "\n",
    "        Returns:\n",
    "            (success: bool, reason: str)\n",
    "        \"\"\"\n",
    "        # âœ” PATTERN MATCHING FOR KNOWN REASONING MODELS (FAST PATH)\n",
    "        known_reasoning_models = [\n",
    "            \"deepseek-r1\",\n",
    "            \"qwen-qwq\",\n",
    "            \"qwen-r\",\n",
    "            \"o1\",\n",
    "            \"reasoning\",\n",
    "            \"r1-preview\",\n",
    "        ]\n",
    "\n",
    "        for pattern in known_reasoning_models:\n",
    "            if pattern in model.lower():\n",
    "                return True, f\"Model matches known reasoning pattern: '{pattern}'\"\n",
    "\n",
    "        # âœ” ATTEMPT ACTUAL PROBE\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                num_ctx=context_window,\n",
    "                num_predict=512,  # Small token limit for probe\n",
    "                reasoning=True,\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=(\n",
    "                        \"You are a reasoning model. \"\n",
    "                        \"Respond with a single JSON object: \"\n",
    "                        '{\"reasoning_capable\": true}'\n",
    "                    )\n",
    "                ),\n",
    "                HumanMessage(content=\"Confirm reasoning mode support.\"),\n",
    "            ]\n",
    "\n",
    "            response = llm.invoke(messages)\n",
    "\n",
    "            # Check if response indicates reasoning mode\n",
    "            if (\n",
    "                    \"reasoning_capable\" in response.content.lower()\n",
    "                    or \"true\" in response.content.lower()\n",
    "            ):\n",
    "                return True, \"Reasoning mode test passed\"\n",
    "\n",
    "            return False, \"Model responded but reasoning mode unconfirmed\"\n",
    "\n",
    "        except TypeError as e:\n",
    "            if \"reasoning\" in str(e).lower():\n",
    "                return (\n",
    "                    False,\n",
    "                    f\"Model does not support reasoning parameter: {str(e)[:80]}\",\n",
    "                )\n",
    "            return False, f\"Parameter error (reasoning unsupported): {str(e)[:80]}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            error_msg = str(e)[:80]\n",
    "\n",
    "            if any(\n",
    "                    term in error_msg.lower()\n",
    "                    for term in [\"unknown parameter\", \"unexpected\", \"reasoning\"]\n",
    "            ):\n",
    "                return (\n",
    "                    False,\n",
    "                    f\"{error_type}: Model does not support reasoning\",\n",
    "                )\n",
    "\n",
    "            return False, f\"{error_type}: Probe failed - {error_msg}\"\n",
    "\n",
    "    def clear_cache(self, model: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Clear cache entries.\n",
    "\n",
    "        Args:\n",
    "            model: Clear specific model, or None to clear all\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            self.memory_cache.clear()\n",
    "            if self.CACHE_FILE.exists():\n",
    "                self.CACHE_FILE.unlink()\n",
    "            if self.show_progress:\n",
    "                print(\"ðŸ—‘ï¸  Cleared all capability cache\")\n",
    "        else:\n",
    "            keys_to_remove = [k for k in self.memory_cache.keys() if k.startswith(model)]\n",
    "            for key in keys_to_remove:\n",
    "                del self.memory_cache[key]\n",
    "            self._save_persistent_cache()\n",
    "            if self.show_progress:\n",
    "                print(f\"ðŸ—‘ï¸  Cleared cache for: {model} ({len(keys_to_remove)} entries)\")\n",
    "\n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Return cache statistics.\"\"\"\n",
    "        total_entries = len(self.memory_cache)\n",
    "        valid_entries = sum(\n",
    "            1\n",
    "            for entry in self.memory_cache.values()\n",
    "            if self._is_cache_valid(entry)\n",
    "        )\n",
    "        supported = sum(\n",
    "            1\n",
    "            for entry in self.memory_cache.values()\n",
    "            if entry.get(\"supports_reasoning\", False)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_entries\": total_entries,\n",
    "            \"valid_entries\": valid_entries,\n",
    "            \"supported_models\": supported,\n",
    "            \"cache_file\": str(self.CACHE_FILE),\n",
    "            \"ttl_hours\": self.CACHE_TTL_HOURS,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860a95ef-e58f-4407-926a-f65559d9ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded capability cache: 1 models\n",
      "  Cache file: .cache\\reasoning_capabilities.json\n",
      "ðŸ“¦ Reasoning capability [CACHED]: gpt-oss:20b\n",
      "  Result: True\n",
      "  Reason: Reasoning mode test passed\n",
      "ðŸ”§ Reasoning Engine Initialized\n",
      "  Model: gpt-oss:20b\n",
      "  Temperature: 0\n",
      "  Context window: 512\n",
      "  Reasoning supported (probed): True\n",
      "  Probe reason: Reasoning mode test passed\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-oss:20b\"\n",
    "temperature = 0\n",
    "context_window = 512\n",
    "\n",
    "probe = ReasoningCapabilityProbe(show_progress=True)\n",
    "reasoning_supported, reasoning_reason = probe.check_reasoning_support(\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    context_window=context_window,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"ðŸ”§ Reasoning Engine Initialized\")\n",
    "print(f\"  Model: {model}\")\n",
    "print(f\"  Temperature: {temperature}\")\n",
    "print(f\"  Context window: {context_window}\")\n",
    "print(f\"  Reasoning supported (probed): {reasoning_supported}\")\n",
    "print(f\"  Probe reason: {reasoning_reason}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d4786-6581-4d82-83b2-541a448e9570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain langchain-ollama langchain-text-splitters langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2ad1b-0e7b-4568-a357-7af63c96b4d2",
   "metadata": {},
   "source": [
    "### RAG Pipeline with langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model\n",
    "# Ensure ollama is running :\n",
    "#    ollama serve\n",
    "#    ollama pull mistral:7b\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:7b\",  \n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"Large Language Models (LLMs) are AI systems trained on massive text datasets to understand and generate human language. \"\n",
    "    \"They are commonly built using Transformer architectures, which allow them to process context across long sequences of text.\",\n",
    "\n",
    "    \"Natural Language Processing (NLP) focuses on enabling computers to interpret and work with human language. \"\n",
    "    \"Core NLP tasks include text classification, question answering, sentiment analysis, summarization, and translation.\",\n",
    "\n",
    "    \"Python is widely used for building and deploying LLM and NLP systems. \"\n",
    "    \"Popular libraries and frameworks include Hugging Face Transformers for model usage, tokenizers for text processing, \"\n",
    "    \"and vector databases for retrieval-augmented generation (RAG) workflows.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "embeddings = FakeEmbeddings(size=256, model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "        \"You are a helpful assistant. Prioritize answering using the provided context. \"\n",
    "        \"If the context contains the answer, rely on it strictly. \"\n",
    "        \"If the context does not contain the answer, you may use your own verified, \"\n",
    "        \"parametric knowledge — but do NOT make up facts. \"\n",
    "        \"If you are not confident or the information is unknown, say \"\n",
    "        \"'I don't know' or 'The context does not provide this information.'\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    ),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-turn RAG ===\n",
      "Response:  A Large Language Model (LLM) refers to a type of artificial intelligence model that is designed to process and understand human language. It's capable of tasks such as text classification, question answering, and other natural language processing tasks. The context suggests that Python is often used for building and deploying these models, and they are commonly built using Transformer architectures. However, the context does not provide a specific definition or size limit for what constitutes a \"large\" language model. For more precise information about large language models, it would be best to consult additional resources or documentation on the topic.\n",
      "\n",
      "=== Multi-turn Conversation ===\n",
      "User: What is a Large Language Model?\n",
      "Assistant:  A Large Language Model (LLM) refers to an AI system that has been trained on vast amounts of text data. These models are designed to understand and generate human-like text based on the input they receive. The context provided mentions that LLMs can be utilized through libraries such as Hugging Face Transformers, and they rely on text tokenizers for processing during training and generation phases.\n",
      "\n",
      "User: What are common tasks in Natural Language Processing?\n",
      "Assistant:  Common tasks in Natural Language Processing (NLP) include question answering, sentiment analysis, summarization, and translation as mentioned in the context provided.\n",
      "\n",
      "User: Can I use Python to build and deploy LLM applications?\n",
      "Assistant:  Yes, based on the provided context, you can certainly use Python to build and deploy Large Language Model (LLM) applications. Libraries such as Hugging Face Transformers in Python are designed for this purpose, making it easier to use pre-trained models for tasks like question answering, sentiment analysis, summarization, and translation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is a Large Language Model?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is a Large Language Model?\",\n",
    "        \"What are common tasks in Natural Language Processing?\",\n",
    "        \"Can I use Python to build and deploy LLM applications?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bec7-917d-4def-999c-a941cdb55aa1",
   "metadata": {},
   "source": [
    "### With Tools (Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97fb922-8a8e-4636-a057-9b65df999133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool  # Docstring is mandatory as it becomes the tool description given to the LLM\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Ask model\n",
    "ai_msg = llm_with_tools.invoke([HumanMessage(\"What is 5 times 3?\")])\n",
    "\n",
    "# Execute tool\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "result = multiply.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(\"Tool result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc10e59-d13c-493a-a73c-7b8650ae3532",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9397ae8d-360c-4a50-9181-df0c12cfcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming ===\n",
      " Large Language Models, like me, are a type of artificial intelligence (AI) model designed to process and generate human-like text. These models are trained on vast amounts of internet text data, learning from it to understand, predict, and generate human-like responses to a wide range of inputs.\n",
      "\n",
      "The training process involves feeding the model with a large dataset of texts, allowing it to learn patterns, grammar rules, and even some facts about the world. However, it's important to note that these models don't have consciousness or emotions, they simply generate text based on patterns they've learned during training.\n",
      "\n",
      "Large Language Models can be used for various tasks such as answering questions, writing essays, summarizing texts, translating languages, and even creating engaging conversations. They are a significant part of the advancements in AI technology, pushing the boundaries of what machines can do when it comes to natural language understanding and generation.\n",
      "\n",
      "However, it's also important to consider potential challenges and ethical implications associated with these models, such as generating misleading or harmful information if not used responsibly. The development and application of large language models are an ongoing area of research and discussion within the AI community.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming ===\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me about Large Language Models\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cff0d-0644-4594-86a2-46848ae9ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain langchain-ollama langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2ad1b-0e7b-4568-a357-7af63c96b4d2",
   "metadata": {},
   "source": [
    "### RAG Pipeline with langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model (NEW IMPORT)\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"LangChain is a framework for building applications with LLMs.\",\n",
    "    \"It provides tools for agents, memory, and retrieval-augmented generation.\",\n",
    "    \"Ollama lets you run open-source models locally on your machine.\",\n",
    "    \"Python is a versatile programming language for AI and data science.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "embeddings = FakeEmbeddings(size=256, model_name=\"fake-embedding\")\n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer based on the context.\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-turn RAG ===\n",
      "Response: LangChain is a framework designed to help build applications that utilize large language models (LLMs). It provides tools, including support for agents, memory systems, and retrieval-augmented generation techniques, which can enhance the functionality and utility of LLMs in various application scenarios. This framework aims to make it easier for developers to integrate advanced natural language processing capabilities into their projects.\n",
      "\n",
      "=== Multi-turn Conversation ===\n",
      "User: What is LangChain?\n",
      "Assistant: LangChain is a framework designed to help developers build applications that utilize large language models (LLMs). It provides several key tools:\n",
      "\n",
      "1. **Agents**: These are like assistants or automations within your application that can interact with LLMs.\n",
      "2. **Memory**: This component helps maintain context and history, allowing the system to have more meaningful interactions over time.\n",
      "3. **Retrieval-Augmented Generation (RAG)**: This technique combines retrieval of relevant information from external sources with generation capabilities of an LLM to produce better responses.\n",
      "\n",
      "Overall, LangChain simplifies the process of integrating and working with large language models by providing these essential tools in a cohesive framework.\n",
      "\n",
      "User: Can I use it with Python?\n",
      "Assistant: Yes, you can use LangChain with Python! Since Python is a versatile programming language widely used in AI and data science, it's an ideal choice for building applications involving LLMs (large language models). LangChain provides tools that are compatible with Python, allowing you to leverage its ecosystem of libraries and frameworks. This makes it possible to integrate LangChain functionalities into your Python projects effectively.\n",
      "\n",
      "User: How does it relate to Ollama?\n",
      "Assistant: Ollama relates to Python in the context of AI and data science by providing a tool for running open-source language models locally on your machine. This can be particularly useful when working with Python, as you can leverage its versatility and powerful libraries like LangChain to build applications involving large language models (LLMs).\n",
      "\n",
      "Here's how they relate more specifically:\n",
      "\n",
      "1. **Python Integration**: Since Ollama allows running open-source models locally, these models can be integrated into Python projects. This means that developers working in Python can use the computational power of their local machine for inference or training, rather than relying on cloud services.\n",
      "\n",
      "2. **LangChain Compatibility**: With Ollama, you can run models that might be used within a LangChain application. LangChain is built to work with various LLMs, so by running an open-source model locally via Ollama, you could potentially use it as part of a LangChain setup for tasks like agent management, memory handling, and retrieval-augmented generation.\n",
      "\n",
      "3. **Versatility**: Both Python and Ollama offer flexibility in AI development. Python’s extensive library ecosystem supports a wide range of ML and NLP tasks, while Ollama’s local model running capabilities provide an additional layer of control over the execution environment, which can be particularly useful for experimentation or deploying models without relying on external services.\n",
      "\n",
      "In summary, Ollama complements Python by offering a way to locally run open-source LLMs, potentially enhancing the AI development workflow when used alongside tools like LangChain and other Python libraries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is LangChain?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is LangChain?\",\n",
    "        \"Can I use it with Python?\",\n",
    "        \"How does it relate to Ollama?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bec7-917d-4def-999c-a941cdb55aa1",
   "metadata": {},
   "source": [
    "### With Tools (Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97fb922-8a8e-4636-a057-9b65df999133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool  # Docstring is mandatory as it becomes the tool description given to the LLM\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Ask model\n",
    "ai_msg = llm_with_tools.invoke([HumanMessage(\"What is 5 times 3?\")])\n",
    "\n",
    "# Execute tool\n",
    "tool_call = ai_msg.tool_calls[0]\n",
    "result = multiply.invoke(tool_call[\"args\"])\n",
    "\n",
    "print(\"Tool result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc10e59-d13c-493a-a73c-7b8650ae3532",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9397ae8d-360c-4a50-9181-df0c12cfcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Streaming ===\n",
      "LangChain is an open-source library for building and running language models in production. It was created with the goal of making it easier to integrate large pre-trained language models into various applications. Here's a brief overview of what LangChain involves:\n",
      "\n",
      "### Key Features\n",
      "\n",
      "1. **Modular Design**: LangChain provides a modular approach, allowing developers to easily plug in different components such as model servers, tokenizers, and data preprocessors.\n",
      "2. **Integration with Models**: It supports integration with popular large language models (LLMs) like Anthropic CLaude, LLaMA, MPT, and others through various APIs and connectors.\n",
      "3. **Versatility**: LangChain can be used for a wide range of applications, including chatbots, text summarization, code generation, and more.\n",
      "4. **Production Readiness**: The library is designed to handle production-scale deployments with features like rate limiting, model versioning, and logging.\n",
      "\n",
      "### Usage\n",
      "\n",
      "Here's a basic example of how you might use LangChain:\n",
      "\n",
      "1. **Install the Library**:\n",
      "   ```bash\n",
      "   pip install langchain\n",
      "   ```\n",
      "\n",
      "2. **Import Necessary Modules**:\n",
      "   ```python\n",
      "   from langchain import HuggingFaceHub\n",
      "   from langchain.prompts import PromptTemplate\n",
      "   from langchain.chains import LLMChain\n",
      "   ```\n",
      "\n",
      "3. **Load a Model and Use It to Generate Text**:\n",
      "   ```python\n",
      "   model_name = \"google/flan-t5-xl\"\n",
      "   llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\": 0.9})\n",
      "\n",
      "   prompt_template = PromptTemplate(input_variables=[\"input_text\"], template=\"Summarize the following text: {input_text}\")\n",
      "   chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "\n",
      "   input_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\n",
      "   output = chain.run(input_text)\n",
      "   print(output)\n",
      "   ```\n",
      "\n",
      "### Example Output\n",
      "```plaintext\n",
      "A summary of the given text would be: Natural language processing (NLP) deals with the interaction between computer systems and natural languages, covering areas within artificial intelligence, linguistics, and computer science.\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "LangChain is a valuable tool for developers looking to leverage large pre-trained language models in their applications. Its modular design and ease of use make it accessible even for those who are new to working with such models. However, it's important to note that as with any AI technology, responsible use and ethical considerations should be taken into account.\n",
      "\n",
      "For more detailed information, you can refer to the official LangChain documentation or GitHub repository.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Streaming ===\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me about LangChain\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cff0d-0644-4594-86a2-46848ae9ef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

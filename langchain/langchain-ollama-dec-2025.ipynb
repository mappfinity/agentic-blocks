{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa95a3e-ad16-43b2-b373-a83b406f6dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-ollama langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0551f3c6-e427-4655-9aa2-d904c4682a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.embeddings import FakeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b44de9-71b1-40ca-a984-e74850994b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Ollama Chat Model (NEW IMPORT)\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc674084-dc45-4779-9cf3-fd70208b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Documents\n",
    "documents = [\n",
    "    \"LangChain is a framework for building applications with LLMs.\",\n",
    "    \"It provides tools for agents, memory, and retrieval-augmented generation.\",\n",
    "    \"Ollama lets you run open-source models locally on your machine.\",\n",
    "    \"Python is a versatile programming language for AI and data science.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7e51be-26d9-4eaf-bfa2-2609fc552850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_text(\"\\n\".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ef4bd5-01d0-47b1-a1df-f69f2bba45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Vector Store\n",
    "embeddings = FakeEmbeddings(size=256, model_name=\"fake-embedding\")\n",
    "vectorstore = InMemoryVectorStore.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a1e4ba9-d9cd-436b-a61a-70968402e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define RAG Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer based on the context.\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad39333-2b1c-4f67-b1ff-647d0105c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Simple RAG Function\n",
    "def retrieve_and_respond(query: str) -> str:\n",
    "    \"\"\"Retrieve docs → format prompt → invoke LLM\"\"\"\n",
    "    context_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    messages = prompt.format_messages(context=context, query=query)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9539b5e3-3254-4a45-ac93-7fd9c2ee20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Multi-turn Conversation\n",
    "class ConversationPipeline:\n",
    "    def __init__(self, llm, retriever, prompt):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.prompt = prompt\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Handle multi-turn conversation\"\"\"\n",
    "        context_docs = self.retriever.invoke(user_query)\n",
    "        context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Format messages with context and conversation history\n",
    "        messages = prompt.format_messages(context=context, query=user_query)\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        response_text = response.content\n",
    "        \n",
    "        # Store in history\n",
    "        self.chat_history.append({\"user\": user_query, \"assistant\": response_text})\n",
    "        \n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64720b05-c6f9-4e39-ac92-e7915f25dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single-turn RAG ===\n",
      "Response: LangChain is a framework designed to help build applications that utilize large language models (LLMs). It provides tools, functionalities, or methods for integrating agents, memory systems, and retrieval-augmented generation techniques with LLMs. This enables developers to create more sophisticated and context-aware AI applications using these advanced language models.\n",
      "\n",
      "=== Multi-turn Conversation ===\n",
      "User: What is LangChain?\n",
      "Assistant: LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides tools, including support for agents, memory, and retrieval-augmented generation, which help in creating more sophisticated and context-aware AI systems. By leveraging LangChain, developers can build applications that not only generate text based on LLMs but also incorporate additional functionalities such as contextual understanding and the ability to retain and utilize information across interactions.\n",
      "\n",
      "User: Can I use it with Python?\n",
      "Assistant: Yes, you can use LangChain with Python! Since Python is a versatile programming language widely used in AI and data science, it's a great choice for building applications that utilize LangChain. LangChain is designed to work seamlessly with various programming languages, including Python, allowing you to leverage its framework for developing robust applications involving large language models (LLMs).\n",
      "\n",
      "To get started, you can integrate LangChain into your Python projects by installing the necessary libraries and following the documentation provided by LangChain. This will enable you to take advantage of features like agents, memory, and retrieval-augmented generation while building AI applications.\n",
      "\n",
      "Additionally, since Ollama allows running open-source models locally on your machine, you could also integrate Ollama with Python and LangChain if that fits your project requirements. However, ensure compatibility between the versions and functionalities of these tools to achieve optimal results in your development process.\n",
      "\n",
      "User: How does it relate to Ollama?\n",
      "Assistant: Ollama and LangChain serve different but complementary roles in the landscape of AI development, particularly when working with large language models (LLMs) and other open-source models.\n",
      "\n",
      "1. **Ollama**: Ollama is designed to allow users to run open-source models locally on their machines. This means that developers can work directly with these models without relying on cloud services or remote servers. By running the models locally, Ollama enables greater control over data privacy and potentially reduces latency in model inference.\n",
      "\n",
      "2. **LangChain**: LangChain is a framework for building applications with LLMs. It provides tools and libraries to help developers integrate and utilize LLMs more effectively. While Ollama focuses on local execution of models, LangChain focuses on the application development process around these models.\n",
      "\n",
      "The relationship between Ollama and LangChain can be seen in how they support different stages of AI application development:\n",
      "- **Ollama** helps with the deployment aspect by allowing you to run models locally.\n",
      "- **LangChain** supports the application development stage by providing tools for building robust applications that leverage LLMs.\n",
      "\n",
      "Together, these tools can enhance a developer's workflow. For example, after deploying an LLM model using Ollama, developers could use LangChain to build sophisticated applications around this model, leveraging features like agents, memory, and retrieval-augmented generation provided by Ollama.\n",
      "\n",
      "In summary, while Ollama is more about the local execution of models, LangChain is focused on building applications with those models. They are tools that can be used in concert to create powerful AI-driven solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Run Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Single-turn RAG ===\")\n",
    "    result = retrieve_and_respond(\"What is LangChain?\")\n",
    "    print(f\"Response: {result}\\n\")\n",
    "    \n",
    "    print(\"=== Multi-turn Conversation ===\")\n",
    "    conversation = ConversationPipeline(llm, retriever, prompt)\n",
    "    \n",
    "    queries = [\n",
    "        \"What is LangChain?\",\n",
    "        \"Can I use it with Python?\",\n",
    "        \"How does it relate to Ollama?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        response = conversation.chat(query)\n",
    "        print(f\"User: {query}\")\n",
    "        print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36091941-cffb-4227-a48d-c894605add41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
